{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.11.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-1.11.0%2Bcpu-cp38-cp38-linux_x86_64.whl (169.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 169.2 MB 66 kB/s s eta 0:00:01    |█████████▎                      | 49.2 MB 20.7 MB/s eta 0:00:06     |█████████▌                      | 50.4 MB 20.7 MB/s eta 0:00:06     |██████████▊                     | 56.7 MB 20.7 MB/s eta 0:00:06     |███████████▋                    | 61.6 MB 20.7 MB/s eta 0:00:06     |███████████████████▋            | 103.6 MB 18.0 MB/s eta 0:00:04     |██████████████████████▏         | 117.3 MB 21.9 MB/s eta 0:00:03MB/s eta 0:00:02     |████████████████████████████▉   | 152.2 MB 24.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.22.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.2.4)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.24.24-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 21.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.11.0+cpu->-r requirements.txt (line 2)) (4.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 4)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 4)) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting botocore<1.28.0,>=1.27.24\n",
      "  Downloading botocore-1.27.24-py3-none-any.whl (9.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.0 MB 20.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 8.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.24->boto3->-r requirements.txt (line 5)) (1.26.5)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, torch, boto3\n",
      "Successfully installed boto3-1.24.24 botocore-1.27.24 jmespath-1.0.1 s3transfer-0.6.0 torch-1.11.0+cpu\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./single_layer_ann_training_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./single_layer_ann_training_pipeline.py\n",
    "# The above line just writes this cell into a Python file; this is used in the KFP DSL command.\n",
    "\n",
    "# Global Kubeflow Pipelines imports.\n",
    "import kfp\n",
    "import kfp.components as comp\n",
    "\n",
    "\n",
    "def unzip_data(bucket_zipfile_path: str, output_str: comp.OutputPath(str)):\n",
    "    # Imports required for the Pipeline Component.\n",
    "    from io import BytesIO\n",
    "\n",
    "    import zipfile\n",
    "    import boto3\n",
    "    import os\n",
    "\n",
    "    # Download a ZIP file from S3.\n",
    "    path_bucket = 'datakflow'\n",
    "    path_to_move_file = ''\n",
    "\n",
    "    os.makedirs('./data', exist_ok=True)\n",
    "    os.makedirs('./unzipped_data', exist_ok=True)\n",
    "\n",
    "    boto3.resource('s3').Object(path_bucket, bucket_zipfile_path).download_file(Filename='./data/zipfile.zip')\n",
    "\n",
    "    for zip in os.listdir('./data'):\n",
    "        with zipfile.ZipFile(os.path.join('./data', zip), 'r') as file:\n",
    "            file.extractall('./unzipped_data')\n",
    "\n",
    "    # Extract all files out of the ZIP file and write them back to S3.\n",
    "    s3_client = boto3.client('s3')\n",
    "    for file in os.listdir('./unzipped_data'):\n",
    "        output_path = path_to_move_file + file\n",
    "        s3_client.upload_file(\n",
    "            os.path.join('./unzipped_data', file),\n",
    "            path_bucket, output_path\n",
    "        )\n",
    "\n",
    "    # Write the path of the required file into an artifact.\n",
    "    with open(output_str, 'w') as writer:\n",
    "        writer.write(output_path)\n",
    "\n",
    "\n",
    "def read_data(\n",
    "    bucket_name: str, csv_path: comp.InputPath(str), sep: str,\n",
    "    decimal: str, encoding: str, output_csv: comp.OutputPath('CSV')\n",
    "):\n",
    "    # Imports required for the Pipeline Component.\n",
    "    from io import StringIO\n",
    "\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "    import os\n",
    "\n",
    "    # Download the unzipped file from S3 into a CSV string.\n",
    "    with open(csv_path, 'r') as reader:\n",
    "        line = reader.readline()\n",
    "        csv_obj = boto3.client('s3').get_object(Bucket=bucket_name, Key=line)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode(encoding)\n",
    "\n",
    "    # Create a pandas DataFrame out of the CSV string.\n",
    "    df = pd.read_csv(\n",
    "        StringIO(csv_string), sep=sep, decimal=decimal,\n",
    "        error_bad_lines=False, encoding=encoding, usecols=['sequence']\n",
    "    )\n",
    "\n",
    "    # Write the CSV into a Kubeflow Pipelines artifact.\n",
    "    df.to_csv(output_csv, index=True, header=True)\n",
    "\n",
    "\n",
    "def preprocess_data(csv_path: comp.InputPath('CSV'), sequence_json: comp.OutputPath()):\n",
    "    # Imports required for the Pipeline Component.\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    \n",
    "    # Read from the artifact CSV.\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Preprocess the dataset.\n",
    "    df['sequence'] = df['sequence'].replace('[]', np.nan).copy()\n",
    "    mask = ~(df['sequence'].isna())\n",
    "    sequences = df.loc[mask, 'sequence']\n",
    "    df = None\n",
    "    sequences = [eval(sequence) for sequence in sequences]\n",
    "\n",
    "    # Write the preprocessed data into an artifact.\n",
    "    with open(sequence_json, 'w') as f:\n",
    "        json.dump(sequences, f)\n",
    "\n",
    "\n",
    "def model_training(sequence_json: comp.InputPath(), model_art: comp.OutputBinaryFile(bytes)):\n",
    "    # Imports required for the Pipeline Component.\n",
    "    import torch\n",
    "    import json\n",
    "    import boto3\n",
    "\n",
    "    # Read the preprocessed data from the artifact.\n",
    "    with open(sequence_json, 'r') as f:\n",
    "        sequences = json.load(f)\n",
    "\n",
    "    # Setting up Dataset and DataLoader for torch model.\n",
    "    train_size = 9000\n",
    "    X_train = torch.Tensor([sequence[:-1] for sequence in sequences[:train_size] if sequence[-1] > -1])\n",
    "    y_train = torch.Tensor([sequence[-1] for sequence in sequences[:train_size] if sequence[-1] > -1]).long()\n",
    "\n",
    "    # Dataset class.\n",
    "    class LogDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            self.sequences = X\n",
    "            self.labels = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            sequence = self.sequences[idx]\n",
    "            label = self.labels[idx]\n",
    "            return sequence, label\n",
    "\n",
    "    train_dataset = LogDataset(X_train, y_train)\n",
    "\n",
    "    # Hyperparameters.\n",
    "    learning_rate = 2e-3\n",
    "    batch_size = 160\n",
    "    epochs = 13\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Model class: single-layer A.N.N.\n",
    "    class SimpleNN(torch.nn.Module):\n",
    "        def __init__(self, input_size, num_keys):\n",
    "            super(SimpleNN, self).__init__()\n",
    "            self.fc = torch.nn.Linear(input_size, num_keys)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.fc(x)\n",
    "            return out\n",
    "\n",
    "    # Training loop over batches of dataset.\n",
    "    def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "        size = len(dataloader.dataset)\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    # Setting up the training loop.\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    model = SimpleNN(10, 11)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Looping over epochs.\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        print('Done!')\n",
    "\n",
    "    # Saving the model into local and an artifact.\n",
    "    checkpoint = {'state_dict': model.state_dict()}\n",
    "    torch.save(checkpoint, model_art)\n",
    "    torch.save(checkpoint, 'checkpoint.pth')\n",
    "\n",
    "    # Uploading the local file into S3.\n",
    "    path_bucket = 'datakflow'\n",
    "    path_to_move_file = ''\n",
    "\n",
    "    boto3.client('s3').upload_file(\n",
    "        'checkpoint.pth',\n",
    "        path_bucket, 'checkpoint.pth'\n",
    "    )\n",
    "\n",
    "\n",
    "def model_evaluating(\n",
    "    sequence_json: comp.InputPath(), model_art: comp.InputBinaryFile(bytes),\n",
    "    arr_output_path: comp.OutputPath(str), mlpipeline_ui_metadata_path: comp.OutputPath()\n",
    "):\n",
    "    # Imports required for the Pipeline Component.\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import json\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    # Reading the preprocessed data from the artifact.\n",
    "    with open(sequence_json, 'r') as f:\n",
    "        sequences = json.load(f)\n",
    "\n",
    "    # Setting up Dataset and DataLoader for torch model.\n",
    "    train_size = 9000\n",
    "    X_valid = torch.Tensor([sequence[:-1] for sequence in sequences[train_size:] if sequence[-1] > -1])\n",
    "    y_valid = torch.Tensor([sequence[-1] for sequence in sequences[train_size:] if sequence[-1] > -1]).long()\n",
    "\n",
    "    # Dataset class.\n",
    "    class LogDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            self.sequences = X\n",
    "            self.labels = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            sequence = self.sequences[idx]\n",
    "            label = self.labels[idx]\n",
    "            return sequence, label\n",
    "\n",
    "    valid_dataset = LogDataset(X_valid, y_valid)\n",
    "\n",
    "    # Hyperparameters.\n",
    "    batch_size = 64\n",
    "    \n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Inference loop over batches of dataset.\n",
    "    def test_loop(dataloader, model, loss_fn):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        test_loss, correct = 0, 0\n",
    "\n",
    "        targets, preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                pred = model(X)\n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                preds += pred.argmax(1).tolist()\n",
    "                targets += y.tolist()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "        vocab = pd.Series(targets).unique()\n",
    "        confusion_matrix_arr = confusion_matrix(targets, preds)\n",
    "        with open(arr_output_path, 'w') as f:\n",
    "            f.write(str(confusion_matrix_arr))\n",
    "        data = []\n",
    "        for i in range(len(confusion_matrix_arr)):\n",
    "            for j in range(len(confusion_matrix_arr[i])):\n",
    "                data.append({'target': vocab[i], 'predicted': vocab[j], 'count': confusion_matrix_arr[i][j]})\n",
    "        df = pd.DataFrame(data)\n",
    "        source = df.to_csv(index=False)\n",
    "        metadata = {\n",
    "            'outputs': [{\n",
    "                'type': 'confusion_matrix',\n",
    "                'format': 'csv',\n",
    "                'schema': [\n",
    "                    {'name': 'target', 'type': 'CATEGORY'},\n",
    "                    {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                    {'name': 'count', 'type': 'NUMBER'},\n",
    "                ],\n",
    "                'storage': 'inline',\n",
    "                'source': source,\n",
    "                'labels': list(map(str, vocab)),\n",
    "            }]\n",
    "        }\n",
    "\n",
    "        with open(mlpipeline_ui_metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f)\n",
    "\n",
    "    # Model class: single-layer A.N.N.\n",
    "    class SimpleNN(torch.nn.Module):\n",
    "        def __init__(self, input_size, num_keys):\n",
    "            super(SimpleNN, self).__init__()\n",
    "            self.fc = torch.nn.Linear(input_size, num_keys)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.fc(x)\n",
    "            return out\n",
    "\n",
    "    # Loading the model from the artifact.\n",
    "    checkpoint = torch.load(model_art)['state_dict']\n",
    "    model = SimpleNN(10, 11)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    # Setting up the inference loop.\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # The inference loop.\n",
    "    test_loop(valid_dataloader, model, loss_fn)\n",
    "\n",
    "\n",
    "base_img = \"sent2020/kflow1:latest\"  # The base container image to be used by pods running the Components.\n",
    "\n",
    "# Create components from the functions above.\n",
    "unzip_data_op        = kfp.components.create_component_from_func(unzip_data, base_image=base_img)\n",
    "read_data_op         = kfp.components.create_component_from_func(read_data, base_image=base_img)\n",
    "preprocess_data_op   = kfp.components.create_component_from_func(preprocess_data, base_image=base_img)\n",
    "model_training_op    = kfp.components.create_component_from_func(model_training, base_image=base_img)\n",
    "model_evaluating_op  = kfp.components.create_component_from_func(\n",
    "    model_evaluating, base_image=base_img, packages_to_install=['scikit-learn']\n",
    ")\n",
    "\n",
    "# Create the pipeline from the components created above.\n",
    "@kfp.dsl.pipeline(\n",
    "    name='single-layer-ann-training-pipeline',\n",
    "    description='Trains a single-layer A.N.N. to find anomalies in string sequences'\n",
    ")\n",
    "def unzip_and_read_pipeline(\n",
    "    bucket_zipfile_path: str, bucket_name: str,\n",
    "    sep: str, decimal: str, encoding: str\n",
    "):\n",
    "    first_task = unzip_data_op(bucket_zipfile_path)\n",
    "    second_task = read_data_op(bucket_name, first_task.outputs['output_str'], sep, decimal, encoding)\n",
    "    third_task = preprocess_data_op(second_task.outputs['output_csv'])\n",
    "    fourth_task = model_training_op(third_task.outputs['sequence_json'])\n",
    "    fifth_task = model_evaluating_op(third_task.outputs['sequence_json'], fourth_task.outputs['model_art'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "dsl-compile --py single_layer_ann_training_pipeline.py --output single_layer_ann_training_pipeline.yaml\n",
    "# Compilation of the pipeline code into a YAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "gcr.io/arrikto/jupyter-kale:v0.5.0-47-g2427cc9",
   "experiment": {
    "id": "new",
    "name": "kale-test-minikf-1"
   },
   "experiment_name": "kale-test-minikf-1",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "random",
     "algorithmSettings": [
      {
       "name": "random_state",
       "value": "42"
      }
     ]
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "additionalMetricNames": [],
     "goal": 92.5,
     "objectiveMetricName": "accuracy",
     "type": "maximize"
    },
    "parallelTrialCount": 3,
    "parameters": [
     {
      "feasibleSpace": {
       "max": "0.005",
       "min": "0.0005",
       "step": "0.0005"
      },
      "name": "learning_rate",
      "parameterType": "double"
     },
     {
      "feasibleSpace": {
       "max": "192",
       "min": "64",
       "step": "64"
      },
      "name": "batch_size",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "max": "10",
       "min": "5",
       "step": "1"
      },
      "name": "epochs",
      "parameterType": "int"
     }
    ]
   },
   "katib_run": true,
   "pipeline_description": "MiniKF image, from scratch.",
   "pipeline_name": "kale-test-minikf-1",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ba3bdfc9cabfb39c224c1aca23919476133c165e34e27a288d907cb258ba46b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
