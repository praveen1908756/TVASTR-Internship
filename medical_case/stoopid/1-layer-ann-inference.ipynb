{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.11.0+cpu in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.11.0+cpu)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.22.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.2.4)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.24.24)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.11.0+cpu->-r requirements.txt (line 2)) (4.1.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 4)) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 4)) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3->-r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.24 in /opt/conda/lib/python3.8/site-packages (from boto3->-r requirements.txt (line 5)) (1.27.24)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3->-r requirements.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.28.0,>=1.27.24->boto3->-r requirements.txt (line 5)) (1.26.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Pipeline with Grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./single_layer_ann_inference_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./single_layer_ann_inference_pipeline.py\n",
    "# The above line just writes this cell into a Python file; this is used in the KFP DSL command.\n",
    "\n",
    "# Global KFP imports.\n",
    "import kfp\n",
    "import kfp.components as comp\n",
    "\n",
    "\n",
    "def unzip_data(\n",
    "    bucket_zipfile_path: str, bucket_name: str, sep: str,\n",
    "    decimal: str, encoding: str, output_csv: comp.OutputPath('CSV')\n",
    "):\n",
    "    # Imports required for the Pipeline Component.\n",
    "    from io import BytesIO\n",
    "\n",
    "    import pandas as pd\n",
    "    import pathlib\n",
    "    import zipfile\n",
    "    import boto3\n",
    "    import os\n",
    "\n",
    "    # Download a ZIP file from S3.\n",
    "    path_bucket = 'datakflow'\n",
    "    path_to_move_file = ''\n",
    "\n",
    "    os.makedirs('./data', exist_ok=True)\n",
    "    os.makedirs('./unzipped_data', exist_ok=True)\n",
    "\n",
    "    boto3.resource('s3').Object(path_bucket, bucket_zipfile_path).download_file(Filename='./data/zipfile.zip')\n",
    "\n",
    "    # Extract all files out of the ZIP file and write them back to S3.\n",
    "    for zip in os.listdir('./data'):\n",
    "        with zipfile.ZipFile(os.path.join('./data', zip), 'r') as file:\n",
    "            file.extractall('./unzipped_data')\n",
    "\n",
    "    for file in pathlib.Path('./unzipped_data').glob('**/*.csv'):\n",
    "        output_path = path_to_move_file / file\n",
    "\n",
    "    df = pd.read_csv(output_path)\n",
    "\n",
    "    # Write the CSV into an artifact.\n",
    "    df.to_csv(output_csv, index=True, header=True)\n",
    "\n",
    "def preprocess_data(csv_path: comp.InputPath('CSV'), sequence_json: comp.OutputPath()):\n",
    "    # Imports required for the Pipeline Component.\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    \n",
    "    # Read from the artifact CSV.\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Preprocess the dataset.\n",
    "    df['sequence'] = df['sequence'].replace('[]', np.nan).copy()\n",
    "    mask = ~(df['sequence'].isna())\n",
    "    sequences = df.loc[mask, 'sequence']\n",
    "    df = None\n",
    "    sequences = [eval(sequence) for sequence in sequences]\n",
    "\n",
    "    # Write the preprocessed data into an artifact.\n",
    "    with open(sequence_json, 'w') as f:\n",
    "        json.dump(sequences, f)\n",
    "\n",
    "def model_inferencing(sequence_json: comp.InputPath(), preds_json: comp.OutputPath()):\n",
    "    # Imports required for the Pipeline Component.\n",
    "    import boto3\n",
    "    import torch\n",
    "    import json\n",
    "\n",
    "    # Read the preprocessed data from the artifact.\n",
    "    with open(sequence_json, 'r') as f:\n",
    "        sequences = json.load(f)\n",
    "\n",
    "    # Setting up Dataset and DataLoader for torch model.\n",
    "    X_valid = torch.Tensor([sequence for sequence in sequences])\n",
    "\n",
    "    # Dataset class.\n",
    "    class LogDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            self.sequences = X\n",
    "            self.labels = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            sequence = self.sequences[idx]\n",
    "            label = self.labels[idx]\n",
    "            return sequence, label\n",
    "\n",
    "    valid_dataset = LogDataset(X_valid, [-1] * len(X_valid))\n",
    "\n",
    "    # Hyperparameters.\n",
    "    batch_size = 64\n",
    "\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Inference loop over batches of dataset.\n",
    "    def test_loop(dataloader, model):\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                try:\n",
    "                    pred = model(X)\n",
    "                    preds += pred.argmax(1).type(torch.float).tolist()\n",
    "                except RuntimeError as e:\n",
    "                    print('Could not ')\n",
    "        return preds\n",
    "\n",
    "    # Model class: single-layer A.N.N.\n",
    "    class SimpleNN(torch.nn.Module):\n",
    "        def __init__(self, input_size, num_keys):\n",
    "            super(SimpleNN, self).__init__()\n",
    "            self.fc = torch.nn.Linear(input_size, num_keys)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.fc(x)\n",
    "            return out\n",
    "\n",
    "    # Downloading the model from S3.\n",
    "    path_bucket = 'datakflow'\n",
    "    boto3.client('s3').download_file(path_bucket, 'checkpoint.pth', 'checkpoint.pth')\n",
    "\n",
    "    # Loading the model from local.\n",
    "    checkpoint = torch.load('checkpoint.pth')['state_dict']\n",
    "    model = SimpleNN(10, 11)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    # The inference loop.\n",
    "    preds = test_loop(valid_dataloader, model)\n",
    "    with open(preds_json, 'w') as f:\n",
    "        json.dump(preds, f)\n",
    "\n",
    "def writing_monitoring_info(csv_path: comp.InputPath('CSV'), preds_json: comp.InputPath()):\n",
    "    # Imports required for the Pipeline Component.\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import requests\n",
    "    import datetime\n",
    "    import json\n",
    "\n",
    "    # Read from the artifact CSV.\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    with open(preds_json) as f:\n",
    "        preds = json.load(f)\n",
    "\n",
    "    # Preprocess the dataset.\n",
    "    df['sequence'] = df['sequence'].replace('[]', np.nan).copy()\n",
    "    mask = ~(df['sequence'].isna())\n",
    "    reqd_columns = ['timestamp', 'danger', 'variables', 'sequence', 'target']\n",
    "    df = df.loc[mask, reqd_columns].reset_index(drop=True)\n",
    "    df['prediction'] = preds\n",
    "\n",
    "    class NumpyEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.void):\n",
    "                return None\n",
    "\n",
    "            if isinstance(obj, (np.generic, np.bool_)):\n",
    "                return obj.item()\n",
    "\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "\n",
    "            return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "    def send_data(data) -> None:\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"http://a055ffc26004049f1ac02141e4c98fb4-409861333.us-east-1.elb.amazonaws.com:8085/iterate/log_1_layer_ann\",\n",
    "                data=json.dumps(data, cls=NumpyEncoder),\n",
    "                headers={\"content-type\": \"application/json\"},\n",
    "            )\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Success.\")\n",
    "\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Got an error code {response.status_code} for the data chunk. \"\n",
    "                    f\"Reason: {response.reason}, error text: {response.text}\"\n",
    "                )\n",
    "\n",
    "        except requests.exceptions.ConnectionError as error:\n",
    "            print(f\"Cannot reach a metrics application, error: {error}, data: {data}\")\n",
    "\n",
    "    data = df.to_dict(orient='records')\n",
    "    send_data(data)\n",
    "\n",
    "base_img = \"sent2020/kflow1:latest\"  # The base container image to be used by pods running the Components.\n",
    "\n",
    "# Create components from the functions above.\n",
    "unzip_data_op              = kfp.components.create_component_from_func(unzip_data, base_image=base_img)\n",
    "preprocess_data_op         = kfp.components.create_component_from_func(preprocess_data, base_image=base_img)\n",
    "model_inferencing_op       = kfp.components.create_component_from_func(model_inferencing, base_image=base_img)\n",
    "writing_monitoring_info_op = kfp.components.create_component_from_func(\n",
    "    writing_monitoring_info, base_image=base_img, packages_to_install=['requests']\n",
    ")\n",
    "\n",
    "# Create the pipeline from the components created above.\n",
    "@kfp.dsl.pipeline(\n",
    "    name='single-layer-ann-inference-pipeline',\n",
    "    description='Performs inference using a single-layer A.N.N. to find anomalies in string sequences'\n",
    ")\n",
    "def unzip_and_read_pipeline(\n",
    "    bucket_zipfile_path, bucket_name,\n",
    "    sep, decimal, encoding\n",
    "):\n",
    "    first_task = unzip_data_op(bucket_zipfile_path, bucket_name, sep, decimal, encoding)\n",
    "    second_task = preprocess_data_op(first_task.outputs['output_csv'])\n",
    "    third_task = model_inferencing_op(second_task.outputs['sequence_json'])\n",
    "    fourth_task = writing_monitoring_info_op(first_task.outputs['output_csv'], third_task.outputs['preds_json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "dsl-compile --py single_layer_ann_inference_pipeline.py --output single_layer_ann_inference_pipeline.yaml\n",
    "# Compilation of the pipeline code into a YAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "gcr.io/arrikto/jupyter-kale:v0.5.0-47-g2427cc9",
   "experiment": {
    "id": "new",
    "name": "kale-test-minikf-1"
   },
   "experiment_name": "kale-test-minikf-1",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "random",
     "algorithmSettings": [
      {
       "name": "random_state",
       "value": "42"
      }
     ]
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "additionalMetricNames": [],
     "goal": 92.5,
     "objectiveMetricName": "accuracy",
     "type": "maximize"
    },
    "parallelTrialCount": 3,
    "parameters": [
     {
      "feasibleSpace": {
       "max": "0.005",
       "min": "0.0005",
       "step": "0.0005"
      },
      "name": "learning_rate",
      "parameterType": "double"
     },
     {
      "feasibleSpace": {
       "max": "192",
       "min": "64",
       "step": "64"
      },
      "name": "batch_size",
      "parameterType": "int"
     },
     {
      "feasibleSpace": {
       "max": "10",
       "min": "5",
       "step": "1"
      },
      "name": "epochs",
      "parameterType": "int"
     }
    ]
   },
   "katib_run": true,
   "pipeline_description": "MiniKF image, from scratch.",
   "pipeline_name": "kale-test-minikf-1",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ba3bdfc9cabfb39c224c1aca23919476133c165e34e27a288d907cb258ba46b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
